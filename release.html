<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Vision Language Models as Robotic Toolsmiths">
  <meta name="keywords" content="VLMgineer, VLM, Toolsmiths, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLMgineer: Vision Language Models as Robotic Toolsmiths</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/VLMgineer.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLMgineer: Vision Language Models as Robotic Toolsmiths</h1>
          
          <!-- Authors -->
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://ggao22.github.io/" target="_blank">George Jiayuan Gao</a>*,
            </span>
            <span class="author-block">
              <a href="http://imtianyuli.com/" target="_blank">Tianyu Li</a>*,
            </span>
            <span class="author-block">
              <a href="https://junyaoshi.github.io/" target="_blank">Junyao Shi</a>,
            </span>
            <span class="author-block">
              <a href="https://yihanli126.github.io/" target="_blank">Yihan Li</a>†,
            </span>
            <span class="author-block">
              <a href="https://zizhe.io/" target="_blank">Zizhe Zhang</a>†,
            </span>
            <span class="author-block">
              <a href="https://nbfigueroa.github.io/" target="_blank">Nadia Figueroa</a>,
            </span>
            <span class="author-block">
              <a href="https://www.seas.upenn.edu/~dineshj/" target="_blank">Dinesh Jayaraman</a>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">* and † denote equal contribution</span>
          </div>
          

          <!-- Affiliations -->
          <div class="is-size-4 publication-authors">
            <span class="author-block">University of Pennsylvania</span>
          </div>
          <div class="is-size-4 publication-authors">
            <span class="author-block">Under Review</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Also <strong>Oral Spotlight</strong>✨ at <a href="https://rss-hardware-intelligence.github.io/" target="_blank">RSS Workshop on Robot Hardware-Aware Intelligence</a>, 2025</span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://vlmgineer.github.io/release.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://vlmgineer.github.io/release.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video">
          <video poster="" autoplay="" controls="" muted="" loop="" height="100%">
              <source src="./static/videos/vlmgineer-web.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, it is often regarded as a measurable indicator of cognitive intelligence across biological species. While much of today's research on robotics intelligence focuses on generating better control strategies, inventing smarter tools offers a complementary form of physical intelligence: moving the problem-solving onus into the tool's geometry so that control becomes simpler.This motivates us to ask: can today's foundation models offer useful priors to automatically invent—and effectively wield—such tools? We present VLMgineer, a framework that harnesses the creativity of Vision–Language Models (VLMs) together with evolutionary search to co-design physical tools and the control policies that operate them. We evaluate VLMgineer on a diverse benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also consistently outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview of VLMgineer</h2>
        <figure class="image">
          <img src="./static/images/vlmgineer_method.png" alt="Overview of ZeroMimic">
        </figure>
        <br>
        <div class="content has-text-justified">
          <p>
            <strong>VLMgineer</strong> takes unmodified environment source code, environment image, environmental description, and task description as context to zero-shot generate tool and action designs from a VLM.  It then iteratively refines its tool and action designs through a loop of candidate sampling, simulation-based evaluation, and evolution improvement.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Bench. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">RoboToolBench</h2>
        <figure class="image">
          <img src="./static/images/RoboToolBench.png" alt="RoboToolBench">
        </figure>
        <br>
        <div class="content has-text-justified">
          <p>
            <strong>VLMgineer</strong> produces innovative tool designs and their corresponding actions across 12 diverse tasks in <strong>RoboToolBench</strong> that are challenging to perform using a general-purpose robot arm and gripper.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Quantitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Quantitative Results on RoboToolBench</h2>
        <figure class="image">
          <img src="./static/images/quan.png" alt="RoboToolBench">
        </figure>
        <br>
        <div class="content has-text-justified">
          <p>
            This figure compares the reward of Franka Gripper experiments, 3 Human Prompt experiments, and experiments on our proposed method across 12 tasks. For every method, the bars with the original dark color in the legend indicate the <i>average</i> reward of the five runs, while the bars with a paler color visible above them indicate the <i>best</i> reward over those runs.
          </p>
          <p>
            <strong>VLMgineer</strong> works consistently well across tasks, in terms of both average and best rewards. We dive into interesting individual method comparisons now. As expected, the default Franka Panda two-finger gripper fails on the majority of these tasks. What is perhaps more noteworthy is that <strong>VLMgineer outperforms human-prompting!</strong> This is true across all tasks on both metrics, showing better and more reliable performance. While human prompts occasionally produced strong solutions, their results were less consistent and efficient. In tasks like <tt>CleanTable</tt> and <tt>ScoreGoal</tt>, both approaches reached similar peak rewards, but our method did so with significantly shorter paths.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Qualitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results on RoboToolBench</h2>
        <figure class="image" style="text-align: center; margin: 0;">
          <img src="./static/images/qual.png" 
               alt="RoboToolBench" 
               style="max-width: 80%; height: auto; display: inline-block;">
        </figure>
        <br>
        <div class="content has-text-justified">
          <p>
            This figure presents a qualitative comparison of human-designed tools, RLBench tools, and <strong>VLMgineer</strong> tools on three tasks: <tt>BringCube</tt> (top row), <tt>ScoreGoal</tt> (middle row), and <tt>GatherSpheres</tt> (bottom row).
            Human-designed tools (left column) generally offer suitable forms for task completion; however, <strong>VLMgineer</strong> (right column) creates more specialized features that enhance performance. For instance, in task <tt>ScoreGoal</tt>, our method produces long and bent shapes facilitating simpler, more efficient motions, which the robot just needs to move very little along one axis to hit the puck. On the other hand, the straight tool designed from human prompts would require more careful control of the puck. In <tt>GatherSphere</tt>, our design includes a scoop with side protection and an overhead stripe structure, effectively preventing spheres from bouncing away.
          </p>          
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">  <!-- Following Experiment Time-Lapse structure for content column width -->
        <div class="content">
          <pre><code class="bibtex-code">Coming Soon!</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
      <div class="content has-text-centered">
        <p>
        Website template borrowed from <a href="https://nerfies.github.io/">NeRFies</a>.
        </p>
      </div>
      </div>
    </div>
    </div>
</footer>

</body>
</html>
